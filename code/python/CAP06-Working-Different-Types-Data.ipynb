{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to Look for APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.shell import spark\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/datasets/retail-data/by-day/2010-12-01.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM dfTable LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to Spark Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Booleans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.where(col(\"InvoiceNo\") != \"536365\") \\\n",
    "    .select(\"InvoiceNo\", \"Description\") \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"InvoiceNo = '536365'\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"InvoiceNo <> '536365'\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"UnitPrice\", \"isExpensive\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"Description\", \"UnitPrice\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, pow, round\n",
    "\n",
    "fabricatedQuantity = round(pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5, 2)\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"ROUND(POWER((Quantity * UnitPrice), 2.0) + 5, 2) AS realQuantity\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT CustomerId, ROUND(POWER((Quantity * UnitPrice), 2.0) + 5, 2) AS realQuantity FROM dfTable\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT ROUND(2.5), BROUND(2.5)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df.stat.corr(\"UnitPrice\", \"Quantity\")\n",
    "df.select(corr(\"UnitPrice\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT corr(Quantity, UnitPrice) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) # 2.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.crosstab(\"StockCode\", \"Country\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(monotonically_increasing_id()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "df.select(initcap(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.select(col(\"Description\"),\n",
    "          lower(col(\"Description\")),\n",
    "          upper(col(\"Description\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "df.select(\n",
    "    ltrim(lit(\"     HELLO     \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"     HELLO     \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"     HELLO     \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.select(\n",
    "    translate(col(\"Description\"), \"LEET\", \"1337\"),\n",
    "    col(\"Description\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "    regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "    col(\"Description\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite) \\\n",
    "    .where(\"hasSimpleColor\") \\\n",
    "    .select(\"Description\") \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column) \\\n",
    "            .cast(\"boolean\") \\\n",
    "            .alias(\"is_\" + color_string)\n",
    "\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\")) # has to a be Column type\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\")) \\\n",
    "    .select(\"Description\") \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Dates and Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10) \\\n",
    "    .withColumn(\"today\", current_date()) \\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), \\\n",
    "              date_add(col(\"today\"), 5)) \\\n",
    "              .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)) \\\n",
    "      .select(datediff(col(\"week_ago\"), col(\"today\"))) \\\n",
    "      .show(1)\n",
    "\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\")\n",
    ").select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", to_date(lit(\"2017-01-01\"))) \\\n",
    "    .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "dateFormat = \"yyyy-MM-dd\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date1\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\")\n",
    ")\n",
    "\n",
    "cleanDateDF.createOrReplaceTempView(\"cleanDateTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "          to_date(date1, 'yyyy-MM-dd') AS date1,\n",
    "          to_date(date2, 'yyyy-MM-dd') AS date2,\n",
    "          to_date(date1)\n",
    "    FROM cleanDateTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "cleanDateDF.select(to_timestamp(col(\"date1\"), dateFormat).alias(\"dateF\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\")).alias(\"descriptionCustomer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1 - Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "\n",
    "complexDF.select(\"complex.Description\")\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"))\n",
    "complexDF.select(\"complex.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2 - Array Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"NewColumn\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3 - Array Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \")).alias(\"sizeColumn\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4 - Array Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\").alias(\"columnArray\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5 - Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \")) \\\n",
    "    .withColumn(\"exploded\", explode(col(\"splitted\"))) \\\n",
    "    .select(\"Description\", \"InvoiceNo\", \"exploded\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 6 - Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1,2,3]}}' as jsonString\"\"\")\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(\"jsonString\", \"myJSONKey\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.selectExpr(\n",
    "    \"get_json_object(jsonString, '$.myJSONKey.myJSONValue[1]') as column\",\n",
    "    \"json_tuple(jsonString, 'myJSONKey')\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) AS myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parsedSchema = StructType((\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "))\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) AS myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) \\\n",
    "    .select(from_json(col(\"newJSON\"), parsedSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
