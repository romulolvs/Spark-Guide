{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4f10a1-2530-4f71-88ca-ca9575590a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark's Toolset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9b5808-c347-4b66-8dd9-07ed08283641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets: Type-Safe Structured APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66accf29-005a-4d07-bf31-a48f9c752185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "// Creating Dataset class\n",
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "    \n",
    "    val flightsDF = spark\n",
    "        .read\n",
    "        .parquet(\"/datasets/flight-data/parquet/2010-summary.parquet/\")\n",
    "    \n",
    "    val flights = flightsDF.as[Flight]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea3caff-58b1-44ed-a453-6ce03fa13a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Writing business logic with type-safe functions and DataFrame SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d0a489-aa20-479c-aa5e-ff01ac36973f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Selecting data - Option 1\n",
    "display(\n",
    "  flights\n",
    "    .limit(5)\n",
    "    .filter(\"ORIGIN_COUNTRY_NAME != 'Canada'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37e3ed5-3c97-4bf7-a426-cdab693f39a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Selecting data - Option 2\n",
    "val updatedFlights = flights\n",
    "    .limit(5)\n",
    "    .filter(\"ORIGIN_COUNTRY_NAME != 'Canada'\")\n",
    "    .withColumn(\"count\", $\"count\" + 5)\n",
    "\n",
    "display(updatedFlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a42b0e-3cf8-4b0e-864a-a2c111d5bb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d81d9e-c45d-49cd-9c5e-b9f15553003b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Load the dataset\n",
    "val staticDataFrame = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"/datasets/retail-data/by-day/*.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "val staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6ea758-397e-4bd8-a98a-7bacd50deb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{window, column, desc, col}\n",
    "\n",
    "// Working with window function\n",
    "staticDataFrame\n",
    "    .selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\",\n",
    "        \"InvoiceDate\")\n",
    "    .groupBy(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\n",
    "    .sum(\"total_cost\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d067c632-3060-4dd1-a535-6c06e9555bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "// Working the readStream\n",
    "val streamingDataFrame = spark.readStream\n",
    "    .schema(staticSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"/datasets/retail-data/by-day/*.csv\")\n",
    "\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bb213d-af6f-4fd7-8ed0-d08692219d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Perform a summation in the process\n",
    "val purchaseByCustomerPerHour = streamingDataFrame\n",
    "    .selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\",\n",
    "        \"InvoiceDate\")\n",
    "    .groupBy(\n",
    "        $\"CustomerId\", window($\"InvoiceDate\", \"1 day\"))\n",
    "    .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff3a743-963f-453c-af21-882981da8b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Start the streaming\n",
    "purchaseByCustomerPerHour.writeStream\n",
    "    .format(\"memory\") // memory = store in-memory table\n",
    "    .queryName(\"customer_purchases\") // the name of the in-memory table\n",
    "    .outputMode(\"complete\") // complete = all the counts should be in the table\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e393c04a-a4cd-48e9-84c2-ad605d5067a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Stream started so run query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM customer_purchases\n",
    "    ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1678bea-1f6d-4386-a937-e2b32ef9ec1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Learning with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b582859-4236-42a5-aa85-b4d2821d08f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Analyzing the Dataframe structure\n",
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0739f04e-2832-4499-9af6-36611d8fe614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format\n",
    "\n",
    "// Transforming data into some numerical representation\n",
    "val preppedDataFrame = staticDataFrame\n",
    "    .na.fill(0)\n",
    "    .withColumn(\"day_of_week\", date_format($\"InvoiceDate\", \"EEEE\"))\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2f08e6-2d83-418f-872c-da6bcf1a579c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Split the data into training and test sets\n",
    "val trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-02-01'\")\n",
    "\n",
    "val testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-02-01'\")\n",
    "\n",
    "trainDataFrame.count()\n",
    "testDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1914c700-1c1a-436b-89d9-0a4b9782577f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "// Transforming with StringIndexer\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"day_of_week\")\n",
    "    .setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358b59c1-506c-4dba-8b52-f731342001d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "// Transforming with OneHotEncoder\n",
    "val encoder = new OneHotEncoder()\n",
    "    .setInputCol(\"day_of_week_index\")\n",
    "    .setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7a3356b-88ed-400b-93de-2492f7300136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "// Transforming with VectorAssembler\n",
    "val vectorAssembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"))\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb76b66-22af-471b-abc4-689436aac893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// Creating a pipeline\n",
    "val transformatPipeline = new Pipeline().setStages(Array(indexer, encoder, vectorAssembler))\n",
    "\n",
    "val fittedPipeline = transformatPipeline.fit(trainDataFrame)\n",
    "\n",
    "val transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110fc172-2779-417a-b900-e6d0317b474d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.clustering.KMeans\n",
    "\n",
    "// Training the model\n",
    "val kmeans = KMeans().setK(20)setSeed(1L)\n",
    "\n",
    "val kmModel = kmeans.fit(transformedTraining)\n",
    "\n",
    "kmModel.computeCost(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "544e84fa-63b2-44a4-af68-d5c0cdec40ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Testing the model\n",
    "val transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "\n",
    "kmModel.computeCost(transformedTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dcae268-144d-4328-9f4c-7451ddbcc895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CAP03-Spark-Toolset",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
